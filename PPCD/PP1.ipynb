{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Parcial 1 \n",
    "Estrada montaño Abril Minerva"
   ],
   "id": "516a06aa98abb5ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Problema 1\n",
    "Respecto al artículo Computing machinery and inteligence de Alan Turing, selecciona alguna de las visiones opuestas a la pregunta principal que no sea The Teological Objection y: (5p)\n",
    "- a) descríbela brevemente\n",
    "- b) argumenta si estás a favor o en contra de la misma\n",
    "\n",
    "\n",
    "### a) Descripción  \n",
    "Este argumento sostiene que, aunque una máquina pudiera realizar tareas que parecen inteligentes, no tiene conciencia, sentimientos o experiencias subjetivas (qualia). Según esta visión, pensar no solo implica procesar información o dar respuestas correctas, sino también tener experiencias internas, como la sensación de ser. Un defensor de esta postura es el filósofo John Searle con su experimento mental conocido como la Habitación China, que sugiere que seguir reglas sintácticas no implica conciencia.\n",
    "\n",
    "### b) ¿A favor o en contra?  \n",
    "Estoy en contra de este argumento. Aunque las máquinas actuales no tengan conciencia, no hay evidencia concluyente que indique que esta sea una condición necesaria para pensar. La inteligencia puede abordarse de una manera funcional: si una máquina actúa y responde como si entendiera, ¿no sería lógico decir que pensó esa respuesta? Además, la conciencia sigue siendo un concepto ambiguo incluso en los humanos, y no hay certeza sobre si solo un cerebro biológico puede generar este tipo de experiencia subjetiva. Por tanto, limitar la capacidad de pensar únicamente a sistemas conscientes parece ser una forma de sesgo."
   ],
   "id": "96ef8df82fa0fd05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Problema 2\n",
    "Respecto a la Agenda Nacional Mexicana de Inteligencia Artificial, elige alguna sección contrasta las conclusiones mencionadas con el estado actual de desarrollo en esa área (5p)"
   ],
   "id": "f6b212ef5d8216d7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "La **Agenda Nacional de Inteligencia Artificial** de México reconoce que para avanzar en IA, es clave mejorar la educación y formación\n",
    "\n",
    "1. **Faltan expertos en IA**: Aunque ha habido avances, como nuevos programas en universidades (UNAM, Tec de Monterrey), todavía no hay suficientes profesionales para cubrir la demanda en la industria.\n",
    "  \n",
    "2. **Actualización de programas de estudio**: Algunas universidades ya incluyen IA en varias carreras, pero no se ha implementado en niveles básicos (primaria), y esto genera una brecha digital entre zonas urbanas y rurales.\n",
    "\n",
    "3. **Colaboración entre academia e industria**: Existen proyectos entre universidades y empresas tecnológicas, pero no están bien distribuidos ni organizados a nivel nacional.\n",
    "\n",
    "4. **Infraestructura tecnológica**: Aunque hay laboratorios de IA en algunas universidades, la falta de recursos en otras limita las oportunidades para muchos estudiantes.\n",
    "\n",
    "**En resumen**, México ha avanzado, pero los problemas que identificó la Agenda (como la falta de expertos, infraestructura y actualización de programas) aún son obstáculos importantes para convertirse en un líder en IA. Para solucionarlo, se necesita más inversión y una colaboración más cercana entre la academia y las empresas."
   ],
   "id": "471ea56bdb33080e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Problema 3\n",
    "Selecciona las dos características más importantes para el conjunto de datos Iris y con ellas\n",
    "obtén las funciones discriminantes de: (10p)\n",
    "- a) Un clasificador determinista a priori\n",
    "- b) Un clasificador estadistico a prior"
   ],
   "id": "714023e49e2eece9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T16:56:34.364218Z",
     "start_time": "2024-10-18T16:56:34.034525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el conjunto de datos Iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Seleccionar las dos características más importantes\n",
    "# Elegimos 'petal length' y 'petal width' (índices 2 y 3)\n",
    "X_selected = X[:, [2, 3]]\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Escalar las características\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# a) Clasificador determinista a priori (K-Nearest Neighbors)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "print(\"Accuracy del clasificador determinista (KNN):\", knn.score(X_test_scaled, y_test))\n",
    "\n",
    "# b) Clasificador estadístico a priori (Linear Discriminant Analysis)\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_scaled, y_train)\n",
    "print(\"Accuracy del clasificador estadístico (LDA):\", lda.score(X_test_scaled, y_test))\n",
    "\n",
    "# Visualización de las funciones discriminantes\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot para KNN\n",
    "plt.subplot(121)\n",
    "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
    "                     np.arange(y_min, y_max, 0.1))\n",
    "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, alpha=0.8)\n",
    "plt.title(\"KNN Decision Boundaries\")\n",
    "plt.xlabel(\"Petal length (scaled)\")\n",
    "plt.ylabel(\"Petal width (scaled)\")\n",
    "\n",
    "# Plot para LDA\n",
    "plt.subplot(122)\n",
    "Z = lda.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.4)\n",
    "plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, alpha=0.8)\n",
    "plt.title(\"LDA Decision Boundaries\")\n",
    "plt.xlabel(\"Petal length (scaled)\")\n",
    "plt.ylabel(\"Petal width (scaled)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "4956d40a3e4e095c",
   "execution_count": 47,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7192b2e31a2dbd77"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Problema 4\n",
    "Para la implementación propia del Perceptrón, simula el funcionamiento del método One versus All con ayuda del conjunto de datos Iris y evalúa cada modelo, puede ser con una métrica o una gráfica (10p)"
   ],
   "id": "c0fae5bc404440f5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T15:38:31.592130Z",
     "start_time": "2024-10-18T15:38:30.734375Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Implementación del Perceptrón\n",
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.n_iterations):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                y_predicted = 1 if linear_output >= 0 else 0\n",
    "                \n",
    "                update = self.lr * (y[idx] - y_predicted)\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "                \n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        return np.where(linear_output >= 0, 1, 0)\n",
    "\n",
    "# Cargar y preparar los datos\n",
    "iris = load_iris()\n",
    "X = iris.data[:, [2, 3]]  # Usamos solo longitud y ancho del pétalo\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Implementar One-vs-All\n",
    "perceptrons = []\n",
    "for i in range(3):  # 3 clases en Iris\n",
    "    y_train_binary = np.where(y_train == i, 1, 0)\n",
    "    perceptron = Perceptron()\n",
    "    perceptron.fit(X_train_scaled, y_train_binary)\n",
    "    perceptrons.append(perceptron)\n",
    "\n",
    "# Función para predecir usando One-vs-All\n",
    "def predict_one_vs_all(X):\n",
    "    predictions = np.array([p.predict(X) for p in perceptrons]).T\n",
    "    return np.argmax(predictions, axis=1)\n",
    "\n",
    "# Evaluar el modelo\n",
    "y_pred = predict_one_vs_all(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Visualización\n",
    "x_min, x_max = X_train_scaled[:, 0].min() - 1, X_train_scaled[:, 0].max() + 1\n",
    "y_min, y_max = X_train_scaled[:, 1].min() - 1, X_train_scaled[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
    "                     np.arange(y_min, y_max, 0.02))\n",
    "Z = predict_one_vs_all(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.contourf(xx, yy, Z, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
    "plt.scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], c=y_test, cmap=plt.cm.RdYlBu, edgecolor='black')\n",
    "plt.xlabel('Longitud del pétalo (estandarizada)')\n",
    "plt.ylabel('Ancho del pétalo (estandarizado)')\n",
    "plt.title('Clasificación One-vs-All con Perceptrón para Iris')\n",
    "plt.show()"
   ],
   "id": "4010deb1a9bfb57d",
   "execution_count": 33,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    " # Problema 5\n",
    "Para el conjunto de datos de vinos: (15p)\n",
    "- a) Aplica la regresión logística al conjunto completo reducido y evalúa su rendimiento\n",
    "- b) Con ayuda de PCA, determina el número de componentes que conservan el 75% de la\n",
    "información y realiza la reducción a ese número de componentes\n",
    "- c) Aplica la regresión logística al conjunto reducido con PCA y evalúa su rendimiento\n",
    "- d) Reduce al mismo número de componentes con ayuda de LDA\n",
    "- e) Aplica la regresión logística al conjunto reducido con LDA y evalúa su rendimiento"
   ],
   "id": "36f1c154e726d37f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T16:56:17.261652Z",
     "start_time": "2024-10-18T16:56:16.560147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Cargar el conjunto de datos de vinos\n",
    "wine = load_wine()\n",
    "X = wine.data\n",
    "y = wine.target\n",
    "\n",
    "# Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Escalar las características\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# a) Aplicar la regresión logística al conjunto completo\n",
    "logistic_full = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_full.fit(X_train_scaled, y_train)\n",
    "y_pred_full = logistic_full.predict(X_test_scaled)\n",
    "print(\"Accuracy de la regresión logística en el conjunto completo:\", accuracy_score(y_test, y_pred_full))\n",
    "print(classification_report(y_test, y_pred_full, target_names=wine.target_names))\n",
    "\n",
    "# b) PCA para determinar el número de componentes que conservan el 75% de la información\n",
    "pca = PCA(n_components=0.75, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "print(\"Número de componentes PCA que conservan el 75% de la información:\", pca.n_components_)\n",
    "\n",
    "# c) Aplicar la regresión logística al conjunto reducido con PCA\n",
    "logistic_pca = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = logistic_pca.predict(X_test_pca)\n",
    "print(\"Accuracy de la regresión logística con PCA:\", accuracy_score(y_test, y_pred_pca))\n",
    "print(classification_report(y_test, y_pred_pca, target_names=wine.target_names))\n",
    "\n",
    "# d) Reducir con LDA\n",
    "n_components_lda = min(len(np.unique(y)) - 1, X_train_scaled.shape[1])\n",
    "lda = LDA(n_components=n_components_lda)\n",
    "X_train_lda = lda.fit_transform(X_train_scaled, y_train)\n",
    "X_test_lda = lda.transform(X_test_scaled)\n",
    "print(\"Número de componentes LDA:\", n_components_lda)\n",
    "\n",
    "# e) Aplicar la regresión logística al conjunto reducido con LDA\n",
    "logistic_lda = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_lda.fit(X_train_lda, y_train)\n",
    "y_pred_lda = logistic_lda.predict(X_test_lda)\n",
    "print(\"Accuracy de la regresión logística con LDA:\", accuracy_score(y_test, y_pred_lda))\n",
    "print(classification_report(y_test, y_pred_lda, target_names=wine.target_names))\n",
    "\n",
    "# Visualización de los resultados\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# PCA\n",
    "plt.subplot(131)\n",
    "plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis')\n",
    "plt.title('PCA')\n",
    "plt.xlabel('Primera componente principal')\n",
    "plt.ylabel('Segunda componente principal')\n",
    "\n",
    "# LDA\n",
    "plt.subplot(132)\n",
    "plt.scatter(X_train_lda[:, 0], X_train_lda[:, 1], c=y_train, cmap='viridis')\n",
    "plt.title('LDA')\n",
    "plt.xlabel('Primera discriminante lineal')\n",
    "plt.ylabel('Segunda discriminante lineal')\n",
    "\n",
    "# Original (primeras dos características)\n",
    "plt.subplot(133)\n",
    "plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=y_train, cmap='viridis')\n",
    "plt.title('Datos originales (2 primeras características)')\n",
    "plt.xlabel('Primera característica')\n",
    "plt.ylabel('Segunda característica')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "599fd61da0a587b3",
   "execution_count": 46,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Problema 6 \n",
    "Elige:\n",
    "- a) Matriz de covarianzas\n",
    "- b) Matriz de dispersión\n",
    "y escribe un código que obtenga la obtenga desde cero"
   ],
   "id": "bae30ac4b348b52c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T14:36:26.231598Z",
     "start_time": "2024-10-18T14:36:26.225755Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Función para calcular la matriz de covarianzas desde cero\n",
    "def calcular_matriz_covarianzas(datos):\n",
    "    # Convertir los datos a un arreglo de NumPy\n",
    "    datos = np.array(datos)\n",
    "    \n",
    "    # Calcular el vector de medias\n",
    "    vector_medias = np.mean(datos, axis=0)\n",
    "    \n",
    "    # Centrar los datos restando el vector de medias\n",
    "    datos_centrados = datos - vector_medias\n",
    "    \n",
    "    # Calcular la matriz de covarianzas\n",
    "    n_muestras = datos.shape[0]\n",
    "    matriz_covarianzas = np.dot(datos_centrados.T, datos_centrados) / (n_muestras - 1)\n",
    "    \n",
    "    return matriz_covarianzas\n",
    "\n",
    "# Ejemplo de uso\n",
    "datos = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "]\n",
    "\n",
    "covarianzas = calcular_matriz_covarianzas(datos)\n",
    "print(\"Matriz de covarianzas:\")\n",
    "print(covarianzas)"
   ],
   "id": "c93c1f34447f0dad",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T14:36:21.976089Z",
     "start_time": "2024-10-18T14:36:21.965240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def calcular_matriz_dispersion(datos):\n",
    "    # Convertir los datos a un arreglo de NumPy\n",
    "    datos = np.array(datos)\n",
    "    \n",
    "    # Calcular el vector de medias\n",
    "    vector_medias = np.mean(datos, axis=0)\n",
    "    \n",
    "    # Centrar los datos restando el vector de medias\n",
    "    datos_centrados = datos - vector_medias\n",
    "    \n",
    "    # Calcular la matriz de dispersión\n",
    "    matriz_dispersion = np.dot(datos_centrados.T, datos_centrados)\n",
    "    \n",
    "    return matriz_dispersion\n",
    "\n",
    "# Ejemplo de uso\n",
    "datos = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "]\n",
    "\n",
    "dispersion = calcular_matriz_dispersion(datos)\n",
    "print(\"Matriz de dispersión:\")\n",
    "print(dispersion)"
   ],
   "id": "39b44d30d225cb39",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pregunta 7\n",
    "Implementa desde cero *Kernel PCA* con la siguiente función de kernel:\n",
    "\n",
    "$$ \\[\n",
    "k\\left( x^{(i)}, x^{(j)} \\right) = \\alpha + \\sum_{l=0}^{n-1} \\left( x^{(i)}_l \\cdot x^{(j)}_l \\right)\n",
    "\\] $$\n",
    "\n",
    "Después prueba su funcionamiento para separar círculos concéntricos; compara los resultados con el uso de PCA sobre los mismos datos. (15p)"
   ],
   "id": "e80f2b6fd9c145d1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T16:55:26.432853Z",
     "start_time": "2024-10-18T16:55:25.418765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# 1. Función de kernel\n",
    "def kernel_function(x_i, x_j, alpha=1.0):\n",
    "    return alpha + np.dot(x_i, x_j)\n",
    "\n",
    "# 2. Implementación de Kernel PCA\n",
    "def kernel_pca(X, n_components=2, alpha=1.0):\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Calcular la matriz de kernel\n",
    "    K = np.zeros((n_samples, n_samples))\n",
    "    for i in range(n_samples):\n",
    "        for j in range(n_samples):\n",
    "            K[i, j] = kernel_function(X[i], X[j], alpha)\n",
    "    \n",
    "    # Centrar la matriz de kernel\n",
    "    one_n = np.ones((n_samples, n_samples)) / n_samples\n",
    "    K_centered = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n",
    "    \n",
    "    # Calcular los autovalores y autovectores\n",
    "    eigvals, eigvecs = np.linalg.eigh(K_centered)\n",
    "    \n",
    "    # Ordenar los autovalores y autovectores en orden descendente\n",
    "    idx = np.argsort(eigvals)[::-1]\n",
    "    eigvals = eigvals[idx]\n",
    "    eigvecs = eigvecs[:, idx]\n",
    "    \n",
    "    # Proyectar los datos\n",
    "    return eigvecs[:, :n_components] * np.sqrt(eigvals[:n_components])\n",
    "\n",
    "# 3. Generar datos de círculos concéntricos\n",
    "X, y = make_circles(n_samples=400, factor=0.3, noise=0.05, random_state=42)\n",
    "\n",
    "# 4. Aplicar PCA estándar y Kernel PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "X_kpca = kernel_pca(X, n_components=2, alpha=1.0)\n",
    "\n",
    "# 5. Visualizar y comparar los resultados\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title('Datos originales')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title('PCA estándar')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y, cmap='viridis', edgecolor='k')\n",
    "plt.title('Kernel PCA')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "d2cb2bb1de73a0b7",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Pregunta 8\n",
    "Elige dos estimadores y demuestra que definen algoritmos escalables (10p):\n",
    "\n",
    "$$ \\[\n",
    "\\bar{x} = n^{-1} \\left[ \\sum_{i=1}^{n} x_{i,1}, \\ldots, \\sum_{i=1}^{n} x_{i,p} \\right]^T\n",
    "\\] $$\n",
    "\n",
    "$$ \\[\n",
    "\\hat{\\sigma}_j^2 = n^{-1} \\sum_{i=1}^{n} x_{i,j}^2 - \\bar{x}_j^2\n",
    "\\] $$ \n",
    "\n",
    "$$ \\[\n",
    "\\hat{\\sigma}_{jk} = n^{-1} \\sum_{i=1}^{n} x_{i,j} x_{i,k} - \\bar{x}_j \\bar{x}_k\n",
    "\\] $$\n"
   ],
   "id": "8db940c42f616ffd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "La media muestral $$\\bar{x}$$ se define como:\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{1}{n} \\left[ \\sum_{i=1}^{n} x_{i,1}, \\dots, \\sum_{i=1}^{n} x_{i,p} \\right]^\\top\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $$x_i = [x_{i,1}, x_{i,2}, \\dots, x_{i,p}]^\\top \\in \\mathbb{R}^p$$ es el vector de la \\(i\\)-ésima observación.\n",
    "- $$n$$ es el número de observaciones.\n",
    "- $$p$$ es el número de dimensiones o características.\n",
    "\n",
    "\n",
    "La varianza muestral en la \\(j\\)-ésima dimensión se define como:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_j^2 = \\frac{1}{n} \\sum_{i=1}^{n} x_{i,j}^2 - \\bar{x}_j^2\n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "- $$x_{i,j}$$ es el valor de la \\(i\\)-ésima observación en la dimensión \\(j\\).\n",
    "- $$\\bar{x}_j$$ es la media muestral en la dimensión \\(j\\), ya calculada.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Consideremos el estimador de la media muestral:\n",
    "\n",
    "$$\n",
    "\\bar{x}_j = \\frac{1}{n} \\sum_{i=1}^{n} x_{i,j}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Para calcular $$\\bar{x}_j$$ en cada dimensión $$j$$, se deben sumar los $$n$$ valores $$x_{i,j}$$. Esto involucra $$n$$ operaciones de suma.\n",
    "\n",
    "Dado que hay $$p$$ dimensiones en total, el número total de operaciones necesarias para calcular $$\\bar{x}$$ es:\n",
    "\n",
    "$$\n",
    "\\text{Número total de operaciones} = O(n \\cdot p)\n",
    "$$\n",
    "\n",
    "\n",
    "El cálculo de la media muestral en cada dimensión $$j$$ es independiente de las demás dimensiones. Esto implica que podemos realizar el cálculo de la media en cada dimensión de manera paralela.\n",
    "\n",
    "Formalmente, podemos dividir el conjunto de observaciones $$S = \\{1, \\dots, n\\}$$ en $$K$$ subconjuntos $$S_k$$ para calcular las sumas parciales en paralelo:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} x_{i,j} = \\sum_{k=1}^{K} \\sum_{i \\in S_k} x_{i,j}\n",
    "$$\n",
    "\n",
    "Cada subconjunto $$S_k$$ puede ser procesado en paralelo, lo que permite reducir el tiempo total de cómputo. En un sistema con $$K$$ procesadores, el tiempo requerido se reduce a $$O\\left(\\frac{n \\cdot p}{K}\\right)$$.\n",
    "\n",
    "\n",
    "\n",
    "El cálculo de la media muestral tiene una complejidad de $$O(n \\cdot p)$$ y es paralelizable de manera eficiente. Por lo tanto, el estimador de la media muestral es **escalable**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Consideremos ahora el estimador de la varianza muestral:\n",
    "\n",
    "$$\n",
    "\\hat{\\sigma}_j^2 = \\frac{1}{n} \\sum_{i=1}^{n} x_{i,j}^2 - \\bar{x}_j^2\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Para calcular $$\\hat{\\sigma}_j^2$$, debemos:\n",
    "\n",
    "Calcular la suma de los cuadrados** de los valores $$x_{i,j}^2$$, lo que requiere $$n$$ operaciones.\n",
    "Restar el cuadrado de la media $$\\bar{x}_j^2$$, que ya fue calculado en el paso anterior.\n",
    "\n",
    "El número total de operaciones necesarias para calcular $$\\hat{\\sigma}_j^2$$ en cada dimensión es $$O(n)$$.\n",
    "\n",
    "Dado que hay $$p$$ dimensiones en total, el número total de operaciones es:\n",
    "\n",
    "$$\n",
    "\\text{Número total de operaciones} = O(n \\cdot p)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "De manera similar al cálculo de la media, el cálculo de $$\\hat{\\sigma}_j^2$$ en cada dimensión $$j$$ es independiente de las demás dimensiones. Esto significa que podemos paralelizar el cálculo de la varianza en las $$p$$ dimensiones.\n",
    "\n",
    "Además, el cálculo de la suma de los cuadrados se puede dividir en subconjuntos $$S_k$$ para procesar en paralelo:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{n} x_{i,j}^2 = \\sum_{k=1}^{K} \\sum_{i \\in S_k} x_{i,j}^2\n",
    "$$\n",
    "\n",
    "Cada subconjunto puede ser procesado en paralelo, lo que reduce el tiempo de cómputo a $$O\\left(\\frac{n \\cdot p}{K}\\right)$$ en un sistema con $$K$$ procesadores.\n",
    "\n",
    "\n",
    "El cálculo de la varianza muestral también tiene una complejidad de $$O(n \\cdot p)$$ y es paralelizable de manera eficiente. Por lo tanto, el estimador de la varianza muestral es escalable.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Ambos estimadores, la media muestral y la varianza muestral por dimensión, tienen una complejidad de $$O(n \\cdot p)$$, lo que significa que el tiempo de cómputo crece de manera lineal con el número de observaciones $$n$$ y la dimensionalidad $$p$$ Además, ambos estimadores son paralelizables, lo que los hace escalables en sistemas con múltiples procesadores.\n",
    "\n",
    "Por lo tanto, estos estimadores definen algoritmos escalables.\n"
   ],
   "id": "e5f8e32faad7cb2f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pregunta 9\n",
    "Escribe un código que obtenga la matriz de momentos aumentada a partir de un conjunto de vectores de datos: **(15p)**\n",
    "\n",
    "$$\n",
    "\\mathbf{A}_{(p+1) \\times (p+1)} = \\sum_{i} \\mathbf{w}_i \\mathbf{w}_i^T =\n",
    "\\begin{bmatrix}\n",
    "n & \\sum x_{i,1} & \\sum x_{i,2} & \\dots & \\sum x_{i,p} \\\\\n",
    "\\sum x_{i,1} & \\sum x_{i,1}^2 & \\sum x_{i,1} x_{i,2} & \\dots & \\sum x_{i,1} x_{i,p} \\\\\n",
    "\\sum x_{i,2} & \\sum x_{i,1} x_{i,2} & \\sum x_{i,2}^2 & \\dots & \\sum x_{i,2} x_{i,p} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sum x_{i,p} & \\sum x_{i,1} x_{i,p} & \\sum x_{i,2} x_{i,p} & \\dots & \\sum x_{i,p}^2\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ],
   "id": "68ecced1c579cff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T14:46:35.971644Z",
     "start_time": "2024-10-18T14:46:35.942913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def matriz_momentos_aumentada(X: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calcula la matriz de momentos aumentada de un conjunto de datos X.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    X : numpy.ndarray\n",
    "        Una matriz 2D de numpy donde las filas son las muestras y las columnas son las características.\n",
    "    \n",
    "    Retorna\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        La matriz de momentos aumentada de X.\n",
    "    \"\"\"\n",
    "    # Paso 1: Obtener dimensiones\n",
    "    n, p = X.shape\n",
    "\n",
    "    # Paso 2: Crear matriz aumentada\n",
    "    W = np.hstack([np.ones((n, 1)), X])\n",
    "\n",
    "    # Paso 3: Calcular la matriz de momentos aumentada\n",
    "    A = W.T @ W\n",
    "\n",
    "    return A\n",
    "\n",
    "# Ejemplo de uso\n",
    "X = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "])\n",
    "\n",
    "matriz_aumentada = matriz_momentos_aumentada(X)\n",
    "print(\"Matriz de Momentos Aumentada:\")\n",
    "print(matriz_aumentada)"
   ],
   "id": "e660b43090f58a28",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4934163491d5de7b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pregunta 10\n",
    "Obten las expresiones para determinar los intervalos de contenedores para histogramas bidimensionales en función del número de contenedores"
   ],
   "id": "d8199dfbcd64ed1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T16:38:59.895774Z",
     "start_time": "2024-10-18T16:38:59.837658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def calcular_intervalos_contenedores(X, num_contenedores):\n",
    "    \"\"\"\n",
    "    Calcula los intervalos de contenedores para un histograma bidimensional.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy.ndarray\n",
    "        Un array 2D donde las filas son las muestras y las columnas son las características.\n",
    "    num_contenedores : int\n",
    "        El número de contenedores deseado en cada dimensión.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Dos arrays que contienen los límites de los contenedores para cada dimensión.\n",
    "    \"\"\"\n",
    "    min_vals = np.min(X, axis=0)\n",
    "    max_vals = np.max(X, axis=0)\n",
    "    intervalos = []\n",
    "\n",
    "    for min_val, max_val in zip(min_vals, max_vals):\n",
    "        intervalos.append(np.linspace(min_val, max_val, num_contenedores + 1))\n",
    "\n",
    "    return intervalos\n",
    "\n",
    "# Ejemplo de uso\n",
    "X = np.random.rand(100, 2)  # Datos aleatorios bidimensionales\n",
    "num_contenedores = 10\n",
    "intervalos = calcular_intervalos_contenedores(X, num_contenedores)\n",
    "\n",
    "print(\"Intervalos para la primera dimensión:\", intervalos[0])\n",
    "print(\"Intervalos para la segunda dimensión:\", intervalos[1])"
   ],
   "id": "297b76f34431d6d8",
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pregunta 11\n",
    " Dentro del paquete sklear. datasets, existe un generador de datos para regresión llamado ma-\n",
    "ke_regression:\n",
    "- a) revisa en la documentación cómo generar un conjunto de datos con los siguientes pará-metros: (15p)\n",
    "50 muestras de 7 características, ruido=20,\n",
    "resultados\n",
    "un valor de tu elección en la semilla del generador de número aleatorios para replicar tus\n",
    "- b) Separa los datos de entrenamiento y pruebas con una semilla establecida para replicar el\n",
    "experimento\n",
    "- c) Aplica regresión lineal al conjunto de entrenamiento\n",
    "- d) Evalúa el resultado sobre el mismo conjunto de entrenamiento y después sobre el de pruebas"
   ],
   "id": "68b1114e1cc961cb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T16:40:25.310369Z",
     "start_time": "2024-10-18T16:40:25.247109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# a) Generamos el conjunto de datos con make_regression\n",
    "n_samples = 50\n",
    "n_features = 7\n",
    "noise = 20\n",
    "random_state = 42  # Semilla para replicabilidad\n",
    "\n",
    "X, y = make_regression(n_samples=n_samples, \n",
    "                       n_features=n_features, \n",
    "                       noise=noise, \n",
    "                       random_state=random_state)\n",
    "\n",
    "print(f\"Forma de X: {X.shape}\")\n",
    "print(f\"Forma de y: {y.shape}\")\n",
    "\n",
    "# b) Separamos los datos en conjuntos de entrenamiento y prueba\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=test_size, \n",
    "                                                    random_state=random_state)\n",
    "\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "print(f\"Forma de X_test: {X_test.shape}\")\n",
    "\n",
    "# c) Aplicamos regresión lineal al conjunto de entrenamiento\n",
    "modelo = LinearRegression()\n",
    "modelo.fit(X_train, y_train)\n",
    "\n",
    "# d) Evaluamos el resultado sobre el conjunto de entrenamiento y de pruebas\n",
    "# Conjunto de entrenamiento\n",
    "y_pred_train = modelo.predict(X_train)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "print(f\"R² en el conjunto de entrenamiento: {r2_train:.4f}\")\n",
    "\n",
    "# Conjunto de pruebas\n",
    "y_pred_test = modelo.predict(X_test)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "print(f\"R² en el conjunto de prueba: {r2_test:.4f}\")"
   ],
   "id": "267a76e97dcac010",
   "execution_count": 41,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pregunta 12\n",
    "Con los datos del problema anterior, escribe un código para comparar el resultado de la regresión lineal estándar y el uso de las regularizaciones Ridge y ElasticNet con al menos dos valores diferentes para los parámetros de regularización (10p)"
   ],
   "id": "d6f13097b96c6bcb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T16:41:43.105057Z",
     "start_time": "2024-10-18T16:41:43.037934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import Ridge, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Regresión lineal estándar (ya la tenemos del problema anterior)\n",
    "print(\"Regresión Lineal Estándar:\")\n",
    "print(f\"R² en el conjunto de prueba: {r2_test:.4f}\")\n",
    "mse_linear = mean_squared_error(y_test, y_pred_test)\n",
    "print(f\"MSE en el conjunto de prueba: {mse_linear:.4f}\\n\")\n",
    "\n",
    "# Ridge Regression con diferentes valores de alpha\n",
    "alphas_ridge = [0.1, 1.0]\n",
    "for alpha in alphas_ridge:\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    y_pred_ridge = ridge.predict(X_test)\n",
    "    r2_ridge = r2_score(y_test, y_pred_ridge)\n",
    "    mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "    print(f\"Ridge Regression (alpha={alpha}):\")\n",
    "    print(f\"R² en el conjunto de prueba: {r2_ridge:.4f}\")\n",
    "    print(f\"MSE en el conjunto de prueba: {mse_ridge:.4f}\\n\")\n",
    "\n",
    "# ElasticNet con diferentes valores de alpha y l1_ratio\n",
    "alphas_elastic = [0.1, 1.0]\n",
    "l1_ratios = [0.5, 0.7]\n",
    "for alpha in alphas_elastic:\n",
    "    for l1_ratio in l1_ratios:\n",
    "        elastic = ElasticNet(alpha=alpha, l1_ratio=l1_ratio)\n",
    "        elastic.fit(X_train, y_train)\n",
    "        y_pred_elastic = elastic.predict(X_test)\n",
    "        r2_elastic = r2_score(y_test, y_pred_elastic)\n",
    "        mse_elastic = mean_squared_error(y_test, y_pred_elastic)\n",
    "        print(f\"ElasticNet (alpha={alpha}, l1_ratio={l1_ratio}):\")\n",
    "        print(f\"R² en el conjunto de prueba: {r2_elastic:.4f}\")\n",
    "        print(f\"MSE en el conjunto de prueba: {mse_elastic:.4f}\\n\")"
   ],
   "id": "dac0eb811a1cd598",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pregunta 13\n",
    "Implementa desde cero el cálculo del estimador:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^T X - \\alpha I)^{-1} X^T y \\quad (5p)\n",
    "$$\n"
   ],
   "id": "f19df8ba29dea7de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T16:42:37.759552Z",
     "start_time": "2024-10-18T16:42:37.726920Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def calcular_estimador(X, y, alpha):\n",
    "    \"\"\"\n",
    "    Calcula el estimador beta según la fórmula:\n",
    "    beta = (X^T X - alpha I)^(-1) X^T y\n",
    "\n",
    "    Parámetros:\n",
    "    X : array de numpy, matriz de características\n",
    "    y : array de numpy, vector de valores objetivo\n",
    "    alpha : float, parámetro de regularización\n",
    "\n",
    "    Retorna:\n",
    "    beta : array de numpy, el estimador calculado\n",
    "    \"\"\"\n",
    "    # Calcular X^T X\n",
    "    XTX = np.dot(X.T, X)\n",
    "    \n",
    "    # Crear la matriz identidad I del tamaño adecuado\n",
    "    I = np.eye(XTX.shape[0])\n",
    "    \n",
    "    # Calcular (X^T X - alpha I)\n",
    "    XTX_reg = XTX - alpha * I\n",
    "    \n",
    "    # Calcular la inversa de (X^T X - alpha I)\n",
    "    XTX_reg_inv = np.linalg.inv(XTX_reg)\n",
    "    \n",
    "    # Calcular X^T y\n",
    "    XTy = np.dot(X.T, y)\n",
    "    \n",
    "    # Calcular el estimador beta\n",
    "    beta = np.dot(XTX_reg_inv, XTy)\n",
    "    \n",
    "    return beta\n",
    "\n",
    "# Ejemplo de uso\n",
    "X = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "y = np.array([2, 4, 6])\n",
    "alpha = 0.1\n",
    "\n",
    "beta_estimado = calcular_estimador(X, y, alpha)\n",
    "print(\"Estimador beta calculado:\")\n",
    "print(beta_estimado)\n",
    "\n",
    "# Comparación con sklearn\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Crear y ajustar un modelo Ridge con el mismo alpha\n",
    "modelo_ridge = Ridge(alpha=alpha, fit_intercept=False)\n",
    "modelo_ridge.fit(X, y)\n",
    "\n",
    "print(\"\\nEstimador beta calculado por sklearn:\")\n",
    "print(modelo_ridge.coef_)\n",
    "\n",
    "# Comparar los resultados\n",
    "print(\"\\nDiferencia entre los estimadores:\")\n",
    "print(np.abs(beta_estimado - modelo_ridge.coef_))"
   ],
   "id": "23bfaf6f16963251",
   "execution_count": 43,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pregunta 14\n",
    "Descarga el archivo que se encuentra en https://bit.ly/3zRrMtH y posteriormente: (15p)\n",
    "- a) determina el grado del polinomio que mejor se ajusta a los datos, la primera columna se considera como dependiente y la segunda como independiente; evalúa su rendimiento\n",
    "- b) posteriormente aplica regresión lineal, obteniendo también su evaluación y comparándola con la obtenida en la regresión"
   ],
   "id": "37dfd7174523dc5f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T15:21:35.047909Z",
     "start_time": "2024-10-18T15:21:34.718442Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv('/Users/aprilapril/Desktop/LCD_5to/PPCD/regression.csv', header=None, names=['y', 'x'])\n",
    "\n",
    "X = data['x'].values.reshape(-1, 1)\n",
    "y = data['y'].values\n",
    "\n",
    "# a) Determinar el grado del polinomio que mejor se ajusta\n",
    "\n",
    "max_degree = 10\n",
    "r2_scores = []\n",
    "\n",
    "for degree in range(1, max_degree + 1):\n",
    "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly_features.fit_transform(X)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    r2_scores.append(r2)\n",
    "\n",
    "best_degree = np.argmax(r2_scores) + 1\n",
    "\n",
    "print(f\"El mejor grado de polinomio es: {best_degree}\")\n",
    "print(f\"R^2 para el mejor grado: {r2_scores[best_degree - 1]}\")\n",
    "\n",
    "# Visualizar los resultados\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, max_degree + 1), r2_scores, marker='o')\n",
    "plt.xlabel('Grado del polinomio')\n",
    "plt.ylabel('R^2 Score')\n",
    "plt.title('R^2 Score vs Grado del polinomio')\n",
    "plt.show()\n",
    "\n",
    "# Ajustar el modelo con el mejor grado\n",
    "best_poly_features = PolynomialFeatures(degree=best_degree, include_bias=False)\n",
    "X_best_poly = best_poly_features.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_best_poly, y, test_size=0.2, random_state=42)\n",
    "\n",
    "best_model = LinearRegression()\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_poly = best_model.predict(X_test)\n",
    "mse_poly = mean_squared_error(y_test, y_pred_poly)\n",
    "r2_poly = r2_score(y_test, y_pred_poly)\n",
    "\n",
    "print(f\"MSE del mejor modelo polinómico: {mse_poly}\")\n",
    "print(f\"R^2 del mejor modelo polinómico: {r2_poly}\")\n",
    "\n",
    "# b) Aplicar regresión lineal simple\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "r2_linear = r2_score(y_test, y_pred_linear)\n",
    "\n",
    "print(f\"\\nMSE de la regresión lineal simple: {mse_linear}\")\n",
    "print(f\"R^2 de la regresión lineal simple: {r2_linear}\")\n",
    "\n",
    "# Comparación visual\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(X, y, color='blue', label='Datos originales')\n",
    "plt.plot(X, linear_model.predict(X), color='red', label='Regresión lineal')\n",
    "\n",
    "X_sorted = np.sort(X, axis=0)\n",
    "X_poly_sorted = best_poly_features.transform(X_sorted)\n",
    "plt.plot(X_sorted, best_model.predict(X_poly_sorted), color='green', label=f'Polinomio grado {best_degree}')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Comparación de modelos')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "998dbf4c05f35d4e",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Pregunta 15\n",
    "Determina el mayor y menor valor (absoluto) decimal que puede representarse en punto flotante\n",
    "de simple precisión (5p)"
   ],
   "id": "f3f4f7c7b1139a78"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-18T15:22:27.627363Z",
     "start_time": "2024-10-18T15:22:27.608190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Mayor valor positivo\n",
    "max_float32 = np.finfo(np.float32).max\n",
    "print(f\"El mayor valor positivo en float32 es: {max_float32}\")\n",
    "\n",
    "# Menor valor positivo normalizado\n",
    "min_normalized_float32 = np.finfo(np.float32).tiny\n",
    "print(f\"El menor valor positivo normalizado en float32 es: {min_normalized_float32}\")\n",
    "\n",
    "# Menor valor positivo denormalizado (subnormal)\n",
    "min_denormalized_float32 = np.nextafter(np.float32(0), np.float32(1))\n",
    "print(f\"El menor valor positivo denormalizado en float32 es: {min_denormalized_float32}\")\n",
    "\n",
    "print(\"\\nEn notación científica:\")\n",
    "print(f\"Mayor valor: {max_float32:.8e}\")\n",
    "print(f\"Menor valor normalizado: {min_normalized_float32:.8e}\")\n",
    "print(f\"Menor valor denormalizado: {min_denormalized_float32:.8e}\")"
   ],
   "id": "48283a78a8effa38",
   "execution_count": 20,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
